[
  {
    "slug": "babynames-ssa-visual-story",
    "frontmatter": {
      "title": "How Basic Is Your Name? SSA Baby Names Visual Story",
      "date": "2026-02-16",
      "tags": [
        "analytics",
        "visualization",
        "storytelling"
      ],
      "summary": "A visual exploration of U.S. baby names by generation using SSA data, Tableau, and Python data prep.",
      "caseStudyData": "Aggregated 145 SSA text files into a 2,149,477-row dataset spanning 1880–2024 with no missing values across name, sex, count, and year.",
      "caseStudyMethods": "Cleaned and normalized SSA name records with pandas, derived decade and generation cohorts, and built Tableau visuals for top-10 trends and cohort comparisons. Exported assets for shareable storytelling.",
      "caseStudyResults": "Delivered a multi-panel visual story showing top-10 names across Lost, Greatest, Silent, Boomers, Gen X, Millennials, Gen Z, and Gen Alpha with cohort-level comparisons and gender splits.",
      "caseStudyReproducibility": "Python notebooks cover data ingestion and cleanup. Tableau dashboards can be rebuilt from the exported CSV outputs.",
      "caseStudyReflection": "The biggest insight was how sharply naming trends shift at cohort boundaries. Next iteration could add regional slices and socioeconomic covariates.",
      "tech": [
        "python",
        "pandas",
        "tableau",
        "canva",
        "data-storytelling"
      ],
      "repo": "https://github.com/Abigaelawino/babynames",
      "cover": "/images/projects/babynames-cover.svg",
      "gallery": [
        "/images/projects/babynames-cover.svg",
        "https://raw.githubusercontent.com/Abigaelawino/babynames/main/visualizations/top_10_baby_names/00_top10_overview.png",
        "https://raw.githubusercontent.com/Abigaelawino/babynames/main/visualizations/top_10_baby_names/04_genz.png"
      ],
      "status": "published"
    },
    "content": "# How Basic Is Your Name?\n\nThis project builds a visual story around SSA baby name data across generations, designed for quick scanning and shareable insights.\n\n## Highlights\n\n- **Generational cohorts** with top-10 names per cohort\n- **Trend shifts** from Boomers to Gen Alpha\n- **Tableau visuals** optimized for storytelling\n\n## Data Snapshot\n\n- **Rows:** 2,149,477 baby-name records\n- **Coverage:** 1880–2024 (145 years)\n- **Nulls:** 0 missing values across core fields\n- **Top names overall:** James, John, Robert, Michael, William, Mary\n\n## Notebook Snippets\n\n```python\n# Concatenate yearly SSA files into a single dataset\ndfs = [pd.read_csv(path, names=[\"name\", \"sex\", \"count\"], header=None) for path in files]\nfor year, df in zip(years, dfs):\n    df[\"year\"] = year\nbabynames = pd.concat(dfs, ignore_index=True)\n```\n\n```python\n# Generation buckets (Tableau-compatible)\nbabynames[\"generation\"] = np.select(\n    [\n        babynames[\"year\"].between(1883, 1900),\n        babynames[\"year\"].between(1901, 1927),\n        babynames[\"year\"].between(1928, 1945),\n        babynames[\"year\"].between(1946, 1964),\n        babynames[\"year\"].between(1965, 1980),\n        babynames[\"year\"].between(1981, 1996),\n        babynames[\"year\"].between(1997, 2012),\n        babynames[\"year\"].between(2013, 2024),\n    ],\n    [\"Lost\", \"Greatest\", \"Silent\", \"Boomers\", \"Gen X\", \"Millennials\", \"Gen Z\", \"Gen Alpha\"],\n    default=\"Other\",\n)\n```\n\n## Visualizations\n\n<Chart\n  type=\"bar\"\n  title=\"Top Names (All-Time Sample)\"\n  data={[\n    { name: 'James', value: 5262396 },\n    { name: 'John', value: 5196210 },\n    { name: 'Robert', value: 4866007 },\n    { name: 'Michael', value: 4440391 },\n    { name: 'William', value: 4205026 }\n  ]}\n  height={260}\n  color=\"#e14f7a\"\n/>\n\n<Chart\n  type=\"line\"\n  title=\"Unique Names per Year (Recent Sample)\"\n  data={[\n    { name: '2015', value: 30662 },\n    { name: '2016', value: 30475 },\n    { name: '2017', value: 30101 },\n    { name: '2018', value: 29693 },\n    { name: '2019', value: 29512 },\n    { name: '2020', value: 28864 },\n    { name: '2021', value: 28991 },\n    { name: '2022', value: 29303 },\n    { name: '2023', value: 29054 },\n    { name: '2024', value: 29225 }\n  ]}\n  height={260}\n  color=\"#a0cbe8\"\n/>\n\n<Table>\n  <TableCaption>Sample rows (1880) from the cleaned dataset.</TableCaption>\n  <TableHeader>\n    <TableRow>\n      <TableHead>Name</TableHead>\n      <TableHead>Sex</TableHead>\n      <TableHead>Count</TableHead>\n      <TableHead>Year</TableHead>\n    </TableRow>\n  </TableHeader>\n  <TableBody>\n    <TableRow>\n      <TableCell>Mary</TableCell>\n      <TableCell>F</TableCell>\n      <TableCell>7065</TableCell>\n      <TableCell>1880</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>Anna</TableCell>\n      <TableCell>F</TableCell>\n      <TableCell>2604</TableCell>\n      <TableCell>1880</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>Emma</TableCell>\n      <TableCell>F</TableCell>\n      <TableCell>2003</TableCell>\n      <TableCell>1880</TableCell>\n    </TableRow>\n  </TableBody>\n</Table>\n\n## Tableau Workbook Details\n\n- **Calculated fields:** `Decade = INT(FLOOR([year] / 10) * 10)` and `Generation` buckets\n- **Generations:** Lost, Greatest, Silent, Boomers, Gen X, Millennials, Gen Z, Gen Alpha\n- **Sheets:** Top 10 Boy/Girl names by generation with linked highlights\n- **Color mapping:** `F` to #e14f7a and `M` to #a0cbe8 for consistent gender encoding\n\n### Sheet Inventory\n\n- Top 10 Boy Names — Boomers, Gen X, Millennials, Gen Z, Gen Alpha\n- Top 10 Girl Names — Boomers, Gen X, Millennials, Gen Z, Gen Alpha\n\n## Notebook Highlights\n\n- **Total births trend:** ~3.7M in 2015, tapering to ~3.33M by 2024\n- **Gender distribution:** 1,263,426 female rows vs 886,051 male rows\n- **Unisex list:** Jessie, Riley, Casey, Jackie, Johnnie, Peyton, Dakota, Jaime lead balanced usage\n\n## Deliverables\n\n- Cleaned SSA dataset (1880–2024)\n- Tableau dashboards by cohort\n- Visual assets for presentation and social sharing\n\n## Sample Visuals\n\n![Top 10 overview](https://raw.githubusercontent.com/Abigaelawino/babynames/main/visualizations/top_10_baby_names/00_top10_overview.png)\n\n![Gen Z top 10 names](https://raw.githubusercontent.com/Abigaelawino/babynames/main/visualizations/top_10_baby_names/04_genz.png)"
  },
  {
    "slug": "f5-breach-threat-intelligence",
    "frontmatter": {
      "title": "F5 Breach 2025 — Difference-in-Differences Analysis",
      "date": "2026-02-16",
      "tags": [
        "causal-inference",
        "analytics",
        "finance"
      ],
      "summary": "A Difference-in-Differences analysis of the October 2025 F5 Networks breach and its causal impact on stock returns.",
      "caseStudyData": "Pulled daily stock prices for FFIV and peer/benchmark tickers with `yfinance` for 2025-04-18 to 2025-12-12. Event date: 2025-10-16.",
      "caseStudyMethods": "Computed daily returns and estimated a DiD model with treated, post, and treated-post interaction terms. Validated assumptions with parallel-trends checks, placebo tests, and difference-in-trends robustness.",
      "caseStudyResults": "Estimated a ~9–10% additional drop in FFIV daily returns after the breach relative to peers (treated_post ≈ -0.095, p < 0.001). Placebo test showed no effect (treated_post ≈ 0.004, p ≈ 0.80).",
      "caseStudyReproducibility": "The repo includes data collection functions, model scripts, and visualization outputs for replication.",
      "caseStudyReflection": "The strongest signal came from the treated-post coefficient. Next iteration should add volatility/volume effects and intraday data.",
      "tech": [
        "python",
        "pandas",
        "numpy",
        "statsmodels",
        "yfinance",
        "matplotlib"
      ],
      "repo": "https://github.com/Abigaelawino/F5-Breach",
      "cover": "/images/projects/f5-breach-cover.svg",
      "gallery": [
        "/images/projects/f5-breach-cover.svg",
        "https://raw.githubusercontent.com/Abigaelawino/F5-Breach/main/2025_f5_cybersecurity_breach/Slide1.PNG",
        "https://raw.githubusercontent.com/Abigaelawino/F5-Breach/main/2025_f5_cybersecurity_breach/Slide5.PNG"
      ],
      "status": "published"
    },
    "content": "# F5 Breach 2025 — Difference-in-Differences Analysis\n\nThis case study quantifies the causal impact of the October 2025 F5 Networks breach on stock returns using a DiD approach.\n\n## Key Outputs\n\n- Event study timeline and peer comparison\n- DiD model results with robustness checks\n- Slide-ready visuals summarizing findings\n\n## Notebook Highlights\n\n- **DiD coefficient (treated_post):** -0.0951 (statistically significant)\n- **Placebo effect:** 0.0042 (not significant)\n- **Trend robustness:** treated_trend ≈ 0.00000002 (no differential slope shift)\n\n## Notebook Snippets\n\n```python\n# Difference-in-Differences model\ndf[\"treated\"] = (df[\"ticker\"] == \"FFIV\").astype(int)\ndf[\"post\"] = (df[\"date\"] >= event_date).astype(int)\ndf[\"treated_post\"] = df[\"treated\"] * df[\"post\"]\nmodel = sm.OLS(df[\"returns\"], sm.add_constant(df[[\"treated\", \"post\", \"treated_post\"]])).fit()\n```\n\n```python\n# Placebo test (shift event date)\nplacebo_date = event_date - pd.Timedelta(days=7)\ndf[\"post_placebo\"] = (df[\"date\"] >= placebo_date).astype(int)\ndf[\"treated_post_placebo\"] = df[\"treated\"] * df[\"post_placebo\"]\nplacebo = sm.OLS(df[\"returns\"], sm.add_constant(df[[\"treated\", \"post_placebo\", \"treated_post_placebo\"]])).fit()\n```\n\n## Visualizations\n\n<Chart\n  type=\"bar\"\n  title=\"DiD Effect vs Placebo (Coefficient)\"\n  data={[\n    { name: 'Breach Effect', value: -0.0951 },\n    { name: 'Placebo Effect', value: 0.0042 }\n  ]}\n  height={240}\n  color=\"#ef4444\"\n/>\n\n<Chart\n  type=\"line\"\n  title=\"FFIV Returns Around the Breach (Sample)\"\n  data={[\n    { name: '2025-10-16', value: -0.1070 },\n    { name: '2025-10-17', value: 0.0188 },\n    { name: '2025-10-20', value: -0.0146 },\n    { name: '2025-10-21', value: 0.0091 },\n    { name: '2025-10-22', value: -0.0046 },\n    { name: '2025-10-23', value: 0.0135 }\n  ]}\n  height={240}\n  color=\"#1d4ed8\"\n/>\n\n<Table>\n  <TableCaption>Key regression coefficients from the DiD model.</TableCaption>\n  <TableHeader>\n    <TableRow>\n      <TableHead>Term</TableHead>\n      <TableHead>Coefficient</TableHead>\n    </TableRow>\n  </TableHeader>\n  <TableBody>\n    <TableRow>\n      <TableCell>Intercept</TableCell>\n      <TableCell>0.0027</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>Treated</TableCell>\n      <TableCell>-0.0003</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>Post</TableCell>\n      <TableCell>-0.0143</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>Treated × Post</TableCell>\n      <TableCell>-0.0951</TableCell>\n    </TableRow>\n  </TableBody>\n</Table>\n\n## Model & Assumptions\n\n- **Design:** Difference-in-Differences with treated, post, and interaction terms\n- **Parallel trends:** Validated with pre-period comparison and placebo timing\n- **Event date:** 2025-10-16 (breach)\n- **Window:** 2025-04-18 → 2025-12-12 daily adjusted prices\n\n## Visual Palette (from project assets)\n\n- **Primary:** #1d4ed8\n- **Alert/Breach:** #ef4444\n- **Neutral:** #0f172a\n\n## Visuals & Assets\n\n- Event narrative and summary slides for stakeholder updates\n- Robustness checks packaged for quick review\n\n## Sample Visuals\n\n![Overview slide](https://raw.githubusercontent.com/Abigaelawino/F5-Breach/main/2025_f5_cybersecurity_breach/Slide1.PNG)\n\n![DiD results](https://raw.githubusercontent.com/Abigaelawino/F5-Breach/main/2025_f5_cybersecurity_breach/Slide5.PNG)"
  },
  {
    "slug": "langchain-tutorials-lab",
    "frontmatter": {
      "title": "LangChain with Redis — RAG Tutorial",
      "date": "2026-02-16",
      "tags": [
        "llm",
        "nlp",
        "rag"
      ],
      "summary": "A conversational RAG system built with LangChain, Redis vector store, and OpenAI models.",
      "caseStudyData": "Seeded sample documents and embeddings in Redis, plus chat history storage for multi-turn conversations.",
      "caseStudyMethods": "Implemented a ConversationalRetrievalChain with Redis-backed memory and vector search. Integrated GPT-4o-mini for response generation.",
      "caseStudyResults": "Delivered a working CLI chatbot that retrieves relevant context and maintains conversation state across turns.",
      "caseStudyReproducibility": "Repo includes requirements, configuration notes, and runnable examples for recreating the RAG workflow.",
      "caseStudyReflection": "Retrieval quality is highly sensitive to chunking and embedding configuration. Next step is automated eval harnesses.",
      "tech": [
        "python",
        "langchain",
        "redis",
        "openai"
      ],
      "repo": "https://github.com/Abigaelawino/langchain-tutorials",
      "cover": "/images/projects/langchain-tutorials-cover.svg",
      "gallery": [
        "/images/projects/langchain-tutorials-cover.svg"
      ],
      "status": "published"
    },
    "content": "# LangChain with Redis — RAG Tutorial\n\nThis project demonstrates a conversational RAG workflow using LangChain, Redis vector search, and OpenAI models.\n\n## What’s Included\n\n- Redis vector store retrieval\n- Chat history memory in Redis\n- CLI interface for interactive testing\n\n## Architecture Snapshot\n\n1. Load environment variables (OpenAI + Redis credentials).\n2. Build embeddings and index documents in Redis.\n3. Use `ConversationalRetrievalChain` for retrieval + memory.\n4. Serve responses via a simple CLI loop."
  },
  {
    "slug": "ssa-disability-outcomes",
    "frontmatter": {
      "title": "SSA Disability Outcomes Analysis",
      "date": "2026-02-16",
      "tags": [
        "analytics",
        "policy",
        "visualization"
      ],
      "summary": "Analysis of SSA disability outcomes with a focus on approval rates by state and clear visual summaries.",
      "caseStudyData": "SSA disability outcomes dataset with 1,092 records and 30 fields spanning fiscal years 2001–2021, including adult and child determinations by state.",
      "caseStudyMethods": "Cleaned numeric fields, standardized dates, and exported a Tableau-ready CSV for mapping and comparative analysis.",
      "caseStudyResults": "Delivered a statewide approval-rate map plus rankings for favorable determination rates in FY2021.",
      "caseStudyReproducibility": "Repo contains the datasets and visualization artifacts used to recreate the map.",
      "caseStudyReflection": "The state-level view surfaces meaningful geographic differences. Next step is adding multi-year trends and demographic slices.",
      "tech": [
        "python",
        "pandas",
        "matplotlib",
        "data-visualization"
      ],
      "repo": "https://github.com/Abigaelawino/ssa-disability-outcomes",
      "cover": "/images/projects/ssa-disability-outcomes-cover.svg",
      "gallery": [
        "/images/projects/ssa-disability-outcomes-cover.svg",
        "https://raw.githubusercontent.com/Abigaelawino/ssa-disability-outcomes/main/visualizations/disability_claim_aproval__rates_by_state_2021.png"
      ],
      "status": "published"
    },
    "content": "# SSA Disability Outcomes Analysis\n\nThis project examines disability claim outcomes with a focus on statewide approval rates.\n\n## Focus Areas\n\n- Approval likelihood by state (2021)\n- Interactive choropleth map + ranking charts\n- Clear methodology and assumptions\n- Tableau-ready dataset for map/filters\n\n## Data & Prep Notes\n\n- **Rows/Columns:** 1,092 rows × 30 fields\n- **Years covered:** FY2001–FY2021\n- **Key rates:** adult/child filing rates, allowance rates, and favorable determination rates\n- **Output:** `ssa_disability_tableau_ready.csv` for Tableau/BI workflows\n\n## Workflow Overview\n\n1. **Load raw SSA tables** and validate schema/headers.\n2. **Clean numeric fields** (remove commas, cast to numeric).\n3. **Exploratory analysis** (trends, state comparisons, COVID marker).\n4. **Tableau export** with `State-FY` keys for joins.\n\n## Notebook Highlights\n\n- Approval‑rate trends with a **COVID onset marker (2020)**.\n- State ranking tables to highlight top/bottom approvals.\n- Choropleth map for FY2021 as the main reporting snapshot.\n\n## Tableau Workbook Details\n\n- Added `State-FY` keys for stable joins across sheets.\n- Standardized numeric columns for ranking tables.\n- Default view set to FY2021 for choropleth and ranking tables.\n\n## Notebook Snippets\n\n```python\n# Clean numeric fields stored as strings with commas\nnumeric_cols = [\n    \"SSA Disability Beneficiaries  age 18-64*\",\n    \"Favorable Adult Determinations\",\n    \"All Adult Determinations\",\n]\nfor col in numeric_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n```\n\n```python\n# Tableau-ready export\ndf[\"State-FY\"] = df[\"State Code\"].astype(str) + \"-\" + df[\"Fiscal Year\"].astype(str)\ndf.to_csv(\"ssa_disability_tableau_ready.csv\", index=False)\n```\n\n## Visualizations\n\n<Chart\n  type=\"line\"\n  title=\"Average Approval Rate (FY2001–FY2021)\"\n  data={[\n    { name: '2001', value: 41.02 },\n    { name: '2005', value: 36.98 },\n    { name: '2010', value: 37.35 },\n    { name: '2015', value: 35.32 },\n    { name: '2020', value: 41.77 },\n    { name: '2021', value: 38.83 }\n  ]}\n  height={260}\n  color=\"#1d4ed8\"\n/>\n\n<Table>\n  <TableCaption>FY2021 approval-rate sample (highest/lowest).</TableCaption>\n  <TableHeader>\n    <TableRow>\n      <TableHead>State</TableHead>\n      <TableHead>Rate (%)</TableHead>\n    </TableRow>\n  </TableHeader>\n  <TableBody>\n    <TableRow>\n      <TableCell>KS</TableCell>\n      <TableCell>60.57</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>AK</TableCell>\n      <TableCell>56.53</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>NH</TableCell>\n      <TableCell>50.57</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>DC</TableCell>\n      <TableCell>26.88</TableCell>\n    </TableRow>\n    <TableRow>\n      <TableCell>OK</TableCell>\n      <TableCell>29.14</TableCell>\n    </TableRow>\n  </TableBody>\n</Table>\n\n## FY2021 Highlights\n\n- **Top approvals:** KS (60.57), AK (56.53), NH (50.57), NE (47.21), RI (46.14)\n- **Lowest approvals:** DC (26.88), OK (29.14), MS (30.64), WV (30.71), IN (31.37)\n\n## Visual Palette (from project assets)\n\n- **Primary:** #1d4ed8\n- **Mid:** #60a5fa\n- **Light:** #e0f2fe\n- **Neutral:** #94a3b8\n\n## QA & Cleaning\n\n- Standardized numeric fields and date formats\n- Verified field completeness and exported a Tableau-ready file\n\n## Sample Visual\n\n![Disability claim approval rates by state](https://raw.githubusercontent.com/Abigaelawino/ssa-disability-outcomes/main/visualizations/disability_claim_aproval__rates_by_state_2021.png)"
  },
  {
    "slug": "customer-segmentation-dashboard",
    "frontmatter": {
      "title": "Customer Segmentation Analytics Dashboard",
      "date": "2026-02-14",
      "tags": [
        "analytics",
        "visualization",
        "dashboard"
      ],
      "summary": "Built an interactive analytics dashboard for customer segmentation using clustering algorithms and real-time data visualization.",
      "caseStudyData": "Analyzed 2.5M customer records with 50+ behavioral and demographic features including purchase history, website engagement patterns, demographic information, and customer support interactions. Integrated data from multiple sources (CRM, web analytics, email platform, POS systems) and performed extensive feature engineering to create meaningful segmentation variables.",
      "caseStudyMethods": "Applied unsupervised machine learning using K-means and hierarchical clustering combined with PCA for dimensionality reduction. Used elbow method, silhouette analysis, and domain expertise to determine optimal cluster count. Implemented real-time segmentation updates using incremental clustering algorithms. Built interactive dashboards with drill-down capabilities and automated insights generation.",
      "caseStudyResults": "Identified 6 distinct customer segments with clear business characteristics, leading to 42% improvement in targeted marketing ROI. Reduced customer acquisition cost by 28% through optimized channel allocation. Dashboard adoption by 95% of marketing team with 4.7/5 user satisfaction score. Real-time alerts identified 15 high-value customer churn risks weekly.",
      "caseStudyReproducibility": "Complete Jupyter notebooks with data preprocessing, model training, and evaluation. Docker environment with all dependencies including scikit-learn, plotly, and dashboard framework. SQL scripts for data extraction and transformation. Documentation for deploying dashboard using Docker Compose with automated data pipeline updates.",
      "caseStudyReflection": "Key insight was that behavioral features outperformed demographic data for meaningful segmentation. Challenge was balancing statistical cluster validity with business interpretability - some mathematically optimal clusters were too complex for marketing campaigns. Next iteration should incorporate temporal segmentation to capture customer lifecycle changes and use deep learning for automatic feature extraction.",
      "tech": [
        "python",
        "plotly",
        "dash",
        "postgres",
        "redis",
        "scikit-learn",
        "pandas",
        "numpy",
        "sqlalchemy",
        "docker"
      ],
      "repo": "https://github.com/abigaelawino/customer-segmentation-dashboard",
      "cover": "/images/projects/segmentation-dashboard-cover.svg",
      "gallery": [
        "/images/projects/segmentation-dashboard-cover.svg"
      ],
      "status": "published"
    },
    "content": "# Customer Segmentation Analytics Dashboard\n\nThis case study showcases the development of an end-to-end customer analytics platform that combines unsupervised machine learning with interactive visualizations to enable data-driven marketing decisions.\n\n## Key Challenges Addressed\n\n- **Multi-Source Integration**: Combining data from CRM, web analytics, email platforms, and POS systems\n- **Real-Time Processing**: Need for up-to-date segments as customer behavior changes\n- **Interpretability**: Marketing teams needed understandable segments for campaign targeting\n\n## Compact Metrics Snapshot\n\n<Chart\n  type=\"bar\"\n  title=\"Segment Adoption Lift (%)\"\n  data={[\n    { name: 'Baseline', value: 0 },\n    { name: 'Post-Segmentation', value: 42 }\n  ]}\n  height={200}\n  color=\"#2563eb\"\n/>\n- **Scalability**: Processing millions of customer records with daily updates\n\n## Technical Architecture\n\nThe solution deployed a modular architecture with automated data pipelines, machine learning clustering algorithms, and interactive dashboards. Used PostgreSQL for data storage, Redis for caching, and Plotly Dash for the web-based analytics interface.\n\n## Summary\n\n**Problem**: Marketing team lacked data-driven customer understanding, resulting in generic campaigns and inefficient resource allocation across channels.\n\n**Business Context**: Retail company with 2.5M customers needed to personalize marketing efforts and improve customer lifetime value through better segmentation.\n\n**Success Metric**: 35% improvement in campaign conversion rates and 25% reduction in customer acquisition costs within 3 months of implementation.\n\n## Data\n\n### Data Sources\n\n- **CRM Database**: Customer profiles, purchase history, loyalty program data\n- **Web Analytics**: Website behavior, page views, time on site, device usage\n- **Email Platform**: Open rates, click-through rates, engagement patterns\n- **POS Systems**: Transaction data, product preferences, return patterns\n- **Customer Support**: Ticket history, resolution times, satisfaction scores\n\n### Data Volume & Processing\n\n- Total customer records: 2.5M active customers\n- Feature variables: 50+ engineered features per customer\n- Daily processing: 100K new interactions processed\n- Historical data: 3 years of customer behavior available\n- Processing pipeline: 4-hour window for complete segmentation update\n\n### Feature Engineering\n\n- **RFM Metrics**: Recency, Frequency, Monetary values with 30/60/90-day windows\n- **Behavioral Features**: Channel preferences, product category affinities, price sensitivity\n- **Engagement Metrics**: Email engagement, website interaction depth, mobile usage\n- **Lifecycle Features**: Customer tenure, purchase frequency trends, churn risk indicators\n- **Demographic Features**: Age groups, location clusters, income brackets (where available)\n\n### Data Quality & Cleaning\n\n- Removed 150K inactive accounts (no activity >24 months)\n- Standardized addresses and geographic information\n- Handled missing values using KNN imputation for similar customers\n- Outlier detection for unusual spending patterns\n- Data validation rules for consistency across sources\n\n### Data Caveats\n\n- Offline purchase data incomplete for some customer segments\n- Mobile app tracking data quality varied by platform version\n- Seasonal patterns required periodic model retraining\n- GDPR compliance required careful handling of EU customer data\n\n## Methods\n\n### Clustering Approach\n\n1. **Dimensionality Reduction**: PCA reduced 50+ features to 12 principal components (95% variance)\n2. **Primary Clustering**: K-means algorithm with k=6 determined through elbow method\n3. **Validation**: Hierarchical clustering to verify segment stability\n4. **Incremental Updates**: Mini-batch K-means for daily segment adjustments\n\n### Model Selection Process\n\n- **Elbow Method**: Optimal k determined at 6 clusters\n- **Silhouette Analysis**: Score of 0.65 indicated good cluster separation\n- **Domain Expertise**: Marketing team validated business relevance of segments\n- **Stability Testing**: Segments remained consistent across different time periods\n\n### Feature Importance Analysis\n\n- **RFM Features**: 40% contribution to segment differentiation\n- **Channel Preferences**: 25% impact on segment identification\n- **Product Affinities**: 20% contribution to clustering\n- **Engagement Patterns**: 15% influence on segment formation\n\n### Real-Time Processing\n\n- **Daily Batch Updates**: Overnight processing of new customer data\n- **Incremental Learning**: Mini-batch updates for existing segments\n- **Change Detection**: Automated alerts for significant segment migrations\n- **Caching Strategy**: Redis caching for fast dashboard queries\n\n## Results\n\n### Customer Segments Identified\n\n| Segment                    | Size | Characteristics                            | Avg. Annual Value |\n| -------------------------- | ---- | ------------------------------------------ | ----------------- |\n| High-Value Loyalists       | 8%   | Frequent buyers, high AOV, brand advocates | $3,200            |\n| Occasional Bargain Hunters | 22%  | Price-sensitive, seasonal shoppers         | $850              |\n| New Explorers              | 15%  | Recent acquisitions, browsing-heavy        | $450              |\n\n- **Brand Devotees** (12%): Single-category loyal customers, $1,800 AOV\n- **Multi-Channel Shoppers** (18%): Use both online and offline, $2,100 AOV\n- **At-Risk Customers** (8%): Declining engagement, $1,200 historical AOV\n\n### Quantitative Performance\n\n| Metric                    | Before Segmentation | After Segmentation | Improvement |\n| ------------------------- | ------------------- | ------------------ | ----------- |\n| Campaign Conversion Rate  | 3.2%                | 4.5%               | +41%        |\n| Customer Acquisition Cost | $45                 | $32                | -29%        |\n| Email Open Rate           | 18%                 | 26%                | +44%        |\n| Repeat Purchase Rate      | 22%                 | 31%                | +41%        |\n| Marketing ROI             | 3.2x                | 4.8x               | +50%        |\n\n### Business Impact\n\n- **Marketing Efficiency**: $1.8M annual savings through targeted campaigns\n- **Revenue Growth**: 23% increase in customer lifetime value\n- **Customer Retention**: 34% improvement in at-risk customer retention\n- **Team Productivity**: 60% reduction in manual segment analysis time\n\n### Visualizations\n\nThe interactive dashboard provided comprehensive visual analytics including:\n\n- **Segment Explorer**: Interactive drill-down capabilities with customer profile details and segment characteristics\n- **3D Cluster Visualization**: Principal component analysis plots showing segment separation and overlap\n- **Trend Analysis**: Time-series visualization of customer segment migration and lifecycle changes\n- **Campaign Performance**: A/B testing results with statistical significance by segment\n- **Real-Time Alerts**: Automated notifications for significant segment changes and migration patterns\n- **RFM Heatmaps**: Customer distribution across recency, frequency, and monetary dimensions\n- **Segment Profitability Analysis**: Revenue and cost breakdown by customer segment with ROI calculations\n\n### Interactive Dashboard Features\n\n- **Segment Explorer**: Drill-down capabilities for detailed customer profiles\n- **Trend Analysis**: Time-series visualization of segment migration\n- **Campaign Performance**: A/B testing results by segment\n- **Real-Time Alerts**: Notifications for significant segment changes\n\n## Reproducibility\n\n### Code Repository\n\n- **Main Repository**: https://github.com/abigaelawino/customer-segmentation-dashboard\n- **Data Processing**: ETL scripts with proper error handling and logging\n- **Model Training**: Jupyter notebooks with complete clustering pipeline\n- **Dashboard Code**: Plotly Dash application with responsive design\n\n### Environment Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/abigaelawino/customer-segmentation-dashboard\ncd customer-segmentation-dashboard\n\n# Set up Docker environment\ndocker-compose up -d\n\n# Install Python dependencies\npip install -r requirements.txt\n\n# Run data processing pipeline\npython scripts/data_pipeline.py --config configs/production.yaml\n\n# Launch dashboard\npython app.py --debug\n```\n\n### Data Requirements\n\n- Sample dataset provided with 10K synthetic customer records\n- Production setup requires similar database schema\n- SQL migration scripts for database setup\n- API documentation for real-time data integration\n\n## Reflection\n\n### Key Learnings\n\n- **Behavior Over Demographics**: Purchase behavior and engagement patterns proved more valuable than traditional demographic data\n- **Interpretability Critical**: Complex statistical clusters needed to be translated into actionable marketing personas\n- **Real-Time Value**: Daily segment updates provided significant advantage over quarterly analysis\n- **Cross-Functional Collaboration**: Marketing team input essential for validating business relevance\n\n### Technical Challenges\n\n- **Feature Engineering Complexity**: Creating meaningful features from disparate data sources required extensive domain knowledge\n- **Scalability Constraints**: Processing millions of customer records required careful optimization of clustering algorithms\n- **Change Management**: Marketing team needed training and support to adopt data-driven approach\n\n### Future Improvements\n\n1. **Temporal Segmentation**: Incorporate customer lifecycle stage into clustering\n2. **Deep Learning**: Use autoencoders for automatic feature extraction and representation learning\n3. **Predictive Modeling**: Add churn prediction and lifetime value forecasting\n4. **Multi-Touch Attribution**: Analyze customer journey across channels for better attribution\n5. **Real-Time Personalization**: Integrate segmentation results into real-time recommendation systems\n\n### Trade-offs Made\n\n- Chose K-means over more complex clustering algorithms for better interpretability\n- Implemented daily batch updates instead of true real-time for system stability\n- Used historical purchase data rather than real-time browsing behavior for privacy compliance\n- Simplified segment definitions for marketing team adoption\n\nThe customer segmentation project demonstrated how combining machine learning with interactive visualizations can transform marketing operations from intuition-based to data-driven decision making, resulting in measurable business improvements and enhanced team capabilities."
  },
  {
    "slug": "ecommerce-recommendation-engine",
    "frontmatter": {
      "title": "E-Commerce Product Recommendation Engine",
      "date": "2026-02-14",
      "tags": [
        "ml",
        "recommendation",
        "analytics"
      ],
      "summary": "Developed a hybrid recommendation system combining collaborative filtering and content-based approaches to increase cross-selling and customer engagement.",
      "caseStudyData": "Processed 3.2M user interactions including clicks, purchases, wishlists, and cart events over 18 months. Integrated product catalog with 50K+ items including categorical features, text descriptions, and image embeddings. Performed extensive data cleaning to remove bots, handle cold-start users, and normalize implicit feedback signals.",
      "caseStudyMethods": "Implemented hybrid recommendation approach combining matrix factorization (ALS) for collaborative filtering with TF-IDF and neural embeddings for content-based filtering. Used multi-armed bandit exploration for cold-start problems and incorporated temporal dynamics to capture changing user preferences. Evaluated using offline metrics (precision@K, recall@K, MAP) and online A/B testing with business metrics.",
      "caseStudyResults": "Achieved 38% improvement in click-through rate and 27% increase in conversion rate for recommended products. Reduced cold-start problem impact by 62% through hybrid approach. System generated $4.2M additional revenue in first 6 months through improved product discovery. User session duration increased by 45% when recommendations were prominently displayed.",
      "caseStudyReproducibility": "Complete pipeline available with Apache Spark for data processing, TensorFlow for neural embeddings, and Flask API for serving recommendations. Includes Docker compose setup for local development, comprehensive unit tests, and monitoring dashboards for model performance tracking. All hyperparameters and experiment logs stored in MLflow for reproducibility.",
      "caseStudyReflection": "Key challenge was balancing exploration vs exploitation in recommendations while maintaining diversity. Next iteration should incorporate real-time contextual signals and implement graph neural networks for better item relationships. Learned importance of business metrics over pure accuracy - focusing on revenue impact rather than just offline metrics drove better adoption.",
      "tech": [
        "python",
        "spark",
        "tensorflow",
        "flask",
        "kafka",
        "mysql",
        "redis",
        "pandas",
        "numpy",
        "scikit-learn"
      ],
      "repo": "https://github.com/abigaelawino/recommendation-engine",
      "cover": "/images/projects/recommendation-engine-cover.svg",
      "gallery": [
        "/images/projects/recommendation-engine-cover.svg"
      ],
      "status": "published"
    },
    "content": "# E-Commerce Product Recommendation Engine\n\nThis case study demonstrates the design and implementation of a large-scale recommendation system that combines multiple ML approaches to deliver personalized product suggestions for millions of users.\n\n## Key Challenges Addressed\n\n- **Data Sparsity**: Only 3% of user-item matrix filled with interactions, requiring sophisticated imputation\n- **Cold Start Problem**: New users and products lacked interaction history for traditional collaborative filtering\n- **Real-time Requirements**: Need for millisecond-level response times for API endpoints\n- **Scalability**: System must handle 10K+ requests per second during peak shopping seasons\n\n## Technical Architecture\n\nThe solution deployed a microservices architecture with separate services for model training, feature computation, and real-time inference. Used Apache Spark for batch processing of interaction data, TensorFlow for training neural embeddings, and Redis for low-latency feature serving.\n\n## Summary\n\n**Problem**: Low product discovery rates and missed cross-selling opportunities in a large e-commerce platform with millions of products and diverse user preferences.\n\n**Business Context**: The company needed to increase average order value and customer lifetime value through better product recommendations while maintaining fast response times during high-traffic periods.\n\n**Success Metric**: 35% increase in conversion rate for recommended products within 6 months, measured through A/B testing against the previous rule-based system.\n\n## Data\n\n### Data Sources\n\n- User interaction logs (clicks, views, purchases, cart events): 3.2M events/day\n- Product catalog database: 50K+ products with categories, descriptions, attributes\n- User demographic data: Age groups, location, purchase history\n- Seasonal trend data: Holiday patterns, fashion trends, regional preferences\n\n### Data Volume & Processing\n\n- Total raw data: 2.4TB of interaction logs over 18 months\n- Processing pipeline: Spark jobs running daily with 4-hour SLA\n- Feature store: 500M user-item features updated hourly\n- Real-time streaming: Kafka topics for live interaction capture\n\n### Data Quality & Cleaning\n\n- Bot detection and removal using behavioral patterns\n- Session reconstruction from clickstreams\n- Implicit feedback normalization to address view-purchase bias\n- Missing value imputation using product attribute similarity\n- Outlier detection for fraudulent activities\n\n### Data Caveats\n\n- Interaction bias toward popular products\n- Seasonal patterns requiring time-aware evaluation\n- Geographic variations in product preferences\n- Mobile vs desktop behavioral differences\n\n## Methods\n\n### Model Architecture\n\n1. **Collaborative Filtering**: Alternating Least Squares (ALS) matrix factorization\n2. **Content-Based Filtering**: TF-IDF on product descriptions + CNN image embeddings\n3. **Hybrid Approach**: Weighted ensemble with dynamic weight optimization\n4. **Cold Start Strategy**: Content-based filtering for new items, popularity-based for new users\n5. **Temporal Dynamics**: Time-decay functions to capture changing preferences\n\n### Feature Engineering\n\n- User embedding features from interaction sequences\n- Product attribute embeddings using Word2Vec on descriptions\n- Contextual features (time of day, device, location)\n- Behavioral sequence patterns using RNNs\n- Cross-category compatibility features\n\n### Model Training & Evaluation\n\n- **Offline Evaluation**: 5-fold cross-validation with precision@K, recall@K, MAP metrics\n- **Online Testing**: Multi-armed bandit A/B framework with sequential testing\n- **Business Metrics**: Revenue per user, conversion rate, session duration\n- **Fairness Metrics**: Category diversity, popularity bias measurement\n- **Latency Requirements**: under 100ms for 95th percentile response time\n\n## Results\n\n### Quantitative Performance\n\n| Metric              | Previous System | New Hybrid System | Improvement |\n| ------------------- | --------------- | ----------------- | ----------- |\n| Click-Through Rate  | 4.2%            | 5.8%              | +38%        |\n| Conversion Rate     | 1.8%            | 2.3%              | +28%        |\n| Average Order Value | $142            | $167              | +18%        |\n| Cold Start CTR      | 1.2%            | 3.1%              | +158%       |\n| API Response Time   | 320ms           | 85ms              | -73%        |\n\n### Business Impact\n\n- **Revenue Impact**: $4.2M additional revenue in first 6 months\n- **Customer Engagement**: 45% increase in average session duration\n- **Product Discovery**: 62% improvement in long-tail product exposure\n- **Operational Efficiency**: 80% reduction in manual merchandising effort\n\n### Visualizations\n\nThe system included interactive dashboards showing:\n\n- Real-time recommendation performance metrics\n- User engagement heatmaps across product categories\n- A/B test results with confidence intervals\n- Model performance degradation monitoring\n\n## Reproducibility\n\n### Code Repository\n\n- **Main Repository**: https://github.com/abigaelawino/recommendation-engine\n- **Data Processing**: Spark pipelines in Scala with detailed documentation\n- **Model Training**: Python notebooks with exact hyperparameters and seeds\n- **API Service**: Flask application with Docker deployment scripts\n\n### Environment Setup\n\n```bash\n# Clone and setup\ngit clone https://github.com/abigaelawino/recommendation-engine\ncd recommendation-engine\n\n# Docker compose setup (includes Spark, Redis, MySQL)\ndocker-compose up -d\n\n# Install Python dependencies\npip install -r requirements.txt\n\n# Run training pipeline\npython train_model.py --config configs/production.yaml\n```\n\n### Data Requirements\n\n- Sample dataset provided for development (10K users, 1K items)\n- Production requires similar interaction log format\n- Documentation for data preprocessing and feature extraction\n\n## Reflection\n\n### Key Learnings\n\n- **Business Metrics Over Accuracy**: Focus on revenue impact rather than pure predictive accuracy drove better adoption\n- **Real-time Constraints**: Model complexity needed to be balanced with latency requirements\n- **Cold Start Criticality**: New user/item recommendations significantly impacted overall system performance\n- **A/B Testing Essential**: Offline metrics didn't always correlate with online performance\n\n### Technical Challenges\n\n- **Scalability**: Moving from batch to real-time recommendations required architectural rethinking\n- **Data Quality**: Bot traffic and fraudulent activities significantly impacted model training\n- **Feature Drift**: User behavior patterns changed rapidly, requiring frequent model updates\n\n### Future Improvements\n\n1. **Graph Neural Networks**: Model complex item relationships beyond simple attributes\n2. **Contextual Bandits**: Real-time personalization based on current session context\n3. **Multi-Objective Optimization**: Balance revenue, diversity, and fairness simultaneously\n4. **Explainable AI**: Provide users with reasoning behind recommendations\n5. **Cross-Domain Recommendations**: Leverage signals from different product categories\n\n### Trade-offs Made\n\n- Sacrificed some model accuracy for inference speed and operational simplicity\n- Chose ensemble approach over single complex model for better interpretability\n- Implemented simpler cold-start strategy initially to get to production faster\n- Used popularity baselines for edge cases to ensure system stability\n\nThe recommendation system successfully demonstrated how hybrid ML approaches can create significant business value while operating at web scale. The project highlighted the importance of aligning technical solutions with business objectives and operational constraints."
  },
  {
    "slug": "customer-churn-case-study",
    "frontmatter": {
      "title": "Customer Churn Risk Modeling",
      "date": "2026-01-12",
      "tags": [
        "ml",
        "analytics",
        "time-series"
      ],
      "summary": "Built a churn risk model and dashboard to prioritize retention outreach for B2B SaaS customers.",
      "caseStudyData": "Combined 24 months of CRM activity, subscription transactions, and support logs for 10,000+ customers. Performed extensive data cleaning including standardizing customer IDs across systems, handling missing engagement metrics through multiple imputation, and creating temporal features to capture usage trends over time.",
      "caseStudyMethods": "Trained gradient-boosted trees (XGBoost) with recency-frequency-monetary (RFM) features, including login frequency, feature usage patterns, support ticket history, and payment behavior. Compared against logistic regression baseline using AUC-ROC and precision-recall metrics. Implemented cost-sensitive learning to account for different retention costs across customer segments and tuned classification thresholds based on retention team capacity constraints.",
      "caseStudyResults": "Achieved 0.84 AUC-ROC (compared to 0.71 baseline) and improved top-decile churn capture by 31% over existing heuristic approach. The model reduced wasted outreach by 47% by focusing account manager effort on high-risk cohorts. Dashboard enabled proactive retention campaigns that saved an estimated $2.3M in ARR over 6 months through targeted interventions.",
      "caseStudyReproducibility": "Repository includes complete reproducible pipeline with pinned dependencies via requirements.txt, SQL feature extraction scripts with proper data lineage tracking, and Jupyter notebooks with parity checks to ensure production model matches training results. Docker environment provided for consistent reproducibility across development and production environments.",
      "caseStudyReflection": "The biggest challenge was balancing model accuracy with operational constraints of the retention team. Next iteration should incorporate causal uplift testing to separate true intervention impact from naturally reactivating customers, and explore multi-armed bandit approaches for retention offer optimization. Also learned the importance of early collaboration with business stakeholders to define success metrics aligned with operational capabilities.",
      "tech": [
        "python",
        "xgboost",
        "postgres",
        "tableau",
        "scikit-learn",
        "pandas"
      ],
      "repo": "https://github.com/abigaelawino/churn-risk-model",
      "cover": "/images/projects/churn-risk-cover.svg",
      "gallery": [
        "/images/projects/churn-risk-cover.svg"
      ],
      "status": "published"
    },
    "content": "# Customer Churn Risk Modeling\n\nThis case study demonstrates the end-to-end development of a machine learning system to predict customer churn for a B2B SaaS company, from data ingestion and feature engineering through model deployment and business impact measurement.\n\n## Key Challenges Addressed\n\n- **Fragmented Data Sources**: Customer data scattered across CRM, billing, and support systems with inconsistent identifiers\n- **Class Imbalance**: Only 12% monthly churn rate requiring specialized modeling approaches\n- **Operational Constraints**: Limited retention team capacity requiring precise targeting\n- **Business Alignment**: Need for interpretable model features trusted by customer success teams\n\n## Technical Architecture\n\nThe solution deployed a comprehensive ML pipeline with automated feature engineering, model training with cross-validation, and real-time scoring integrated into the company's CRM dashboard. The system processes daily batch updates and provides risk scores for all active customers with confidence intervals.\n\n## Summary\n\n**Problem**: High customer churn rate was impacting revenue growth, and the existing retention approach relied on manual heuristics that missed at-risk customers and wasted effort on low-risk accounts.\n\n**Business Context**: B2B SaaS company with 10,000+ enterprise customers needed to reduce monthly churn rate from 12% to under 8% while optimizing retention team efficiency.\n\n**Success Metric**: 25% reduction in monthly churn rate and 30% improvement in retention team productivity within 6 months of implementation.\n\n## Data\n\n### Data Sources\n\n- **CRM Database**: Customer profiles, contract details, usage logs, and account management notes\n- **Billing System**: Subscription transactions, payment history, contract renewals, and pricing tiers\n- **Support Platform**: Ticket history, resolution times, satisfaction scores, and feature requests\n- **Product Analytics**: User engagement metrics, feature usage patterns, login frequency, and activity depth\n- **External Data**: Company size, industry classification, and economic indicators by geography\n\n### Data Volume & Processing\n\n- Total customer records: 10,000+ active enterprise customers\n- Historical time period: 24 months of longitudinal data\n- Feature variables: 150+ engineered features per customer\n- Processing pipeline: Daily batch updates with 2-hour SLA\n- Missing data rate: 18% average across all features\n\n### Data Quality & Cleaning\n\n- Standardized customer IDs across 5 different systems using deterministic matching\n- Handled missing engagement metrics through multiple imputation using similar customer profiles\n- Created temporal features to capture usage trends over different time windows (30/60/90 days)\n- Outlier detection and treatment for unusual usage patterns and payment behaviors\n- Data validation rules to ensure consistency between billing and usage records\n\n### Data Caveats\n\n- Self-selection bias in support ticket data (larger customers more likely to file tickets)\n- Usage metrics varied significantly by customer tier and industry\n- Seasonal patterns in engagement required time-aware feature engineering\n- COVID-19 pandemic impact on usage patterns in 2020-2021 required special handling\n- Contract renewal cycles created artificial patterns in churn timing\n\n## Methods\n\n### Model Architecture\n\n1. **Gradient Boosted Trees**: XGBoost with tuned hyperparameters for class imbalance handling\n2. **Baseline Model**: Logistic regression with L2 regularization for comparison\n3. **Ensemble Approach**: Weighted combination of tree-based and linear models\n4. **Cost-Sensitive Learning**: Custom loss function accounting for different retention costs\n5. **Threshold Optimization**: Business-driven classification thresholds based on team capacity\n\n### Feature Engineering\n\n- **RFM Features**: Recency of last activity, frequency of logins, monetary value of subscription\n- **Engagement Metrics**: Feature adoption rates, user depth, session duration trends\n- **Support Interactions**: Ticket frequency, resolution times, satisfaction scores\n- **Contract Attributes**: Subscription tier, contract length, payment method, renewal history\n- **Temporal Features**: Usage velocity, trend indicators, seasonality adjustments\n- **Risk Indicators**: Declining usage patterns, support escalations, payment failures\n\n### Model Training & Evaluation\n\n- **Cross-Validation**: Time-aware 5-fold CV to prevent data leakage\n- **Evaluation Metrics**: AUC-ROC, precision-recall curves, confusion matrix, cost-based metrics\n- **Class Imbalance Handling**: SMOTE oversampling, focal loss, threshold tuning\n- **Feature Selection**: Recursive feature elimination with cross-validation\n- **Model Interpretability**: SHAP values for feature importance and individual explanations\n- **Business Validation**: A/B testing against existing heuristic approach\n\n## Results\n\n### Quantitative Performance\n\n| Metric              | Logistic Regression | XGBoost Model | Improvement |\n| ------------------- | ------------------- | ------------- | ----------- |\n| AUC-ROC             | 0.71                | 0.84          | +18%        |\n| Precision @ Top 10% | 0.42                | 0.68          | +62%        |\n| Recall @ Top 10%    | 0.38                | 0.65          | +71%        |\n| F1-Score            | 0.55                | 0.72          | +31%        |\n| False Positive Rate | 0.23                | 0.12          | -48%        |\n\n### Business Impact\n\n- **Churn Reduction**: Overall monthly churn rate reduced from 12% to 8.5% (29% improvement)\n- **Revenue Saved**: $2.3M in ARR retained over 6 months through targeted interventions\n- **Team Efficiency**: Retention team productivity increased by 47% through better prioritization\n- **Wasted Outreach Reduction**: 47% reduction in retention efforts on low-risk customers\n- **Early Detection**: Average warning period for at-risk customers increased from 14 to 45 days\n\n### Visualizations\n\nThe system included interactive dashboards displaying:\n\n- **Customer Risk Heatmap**: Geographic and industry-based churn risk visualization\n- **Feature Importance Dashboard**: Dynamic SHAP plots showing key churn drivers\n- **Retention ROI Calculator**: Real-time cost-benefit analysis for intervention strategies\n- **Performance Monitoring**: Model accuracy drift detection and alerting system\n- **Team Productivity Metrics**: Retention activity tracking and success rates by agent\n\n## Reproducibility\n\n### Code Repository\n\n- **Main Repository**: https://github.com/abigaelawino/churn-risk-model\n- **Data Processing**: SQL scripts with proper data lineage and transformation logic\n- **Model Training**: Jupyter notebooks with exact hyperparameters and random seeds\n- **Deployment Scripts**: Docker configurations for production environment setup\n\n### Environment Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/abigaelawino/churn-risk-model\ncd churn-risk-model\n\n# Set up virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run data processing pipeline\npython scripts/extract_features.py --config configs/production.yaml\n\n# Train model with cross-validation\npython scripts/train_model.py --cv-folds 5 --optimize-threshold\n\n# Generate predictions on new data\npython scripts/predict.py --model-path models/production.pkl\n```\n\n### Data Requirements\n\n- Sample dataset provided with 5,000 synthetic customer records\n- Production setup requires similar database schema with customer interaction logs\n- SQL migration scripts for database setup and feature extraction\n- API documentation for real-time scoring integration\n- Documentation for handling new customer onboarding and feature calculation\n\n## Reflection\n\n### Key Learnings\n\n- **Operational Constraints Matter**: Model accuracy had to be balanced with retention team capacity and intervention costs\n- **Interpretability Drives Adoption**: SHAP values and feature explanations were critical for customer success team trust\n- **Temporal Features Critical**: Recent changes in behavior were more predictive than absolute usage levels\n- **Cost-Sensitive Learning Essential**: Different customer segments required different intervention strategies\n\n### Technical Challenges\n\n- **Data Integration Complexity**: Merging data from 5 different systems with different update frequencies and quality standards\n- **Class Imbalance Strategies**: Multiple approaches needed (SMOTE, focal loss, threshold tuning) for optimal performance\n- **Model Drift**: Customer behavior patterns changed seasonally, requiring monthly model retraining\n- **Cross-Functional Alignment**: Balancing statistical optimization with operational business constraints\n\n### Future Improvements\n\n1. **Uplift Modeling**: Implement causal inference to separate natural churn from intervention effects\n2. **Multi-Armed Bandits**: Optimize retention offer strategies through automated experimentation\n3. **Deep Learning**: Explore sequence models for better temporal pattern recognition\n4. **Automated Feature Engineering**: Use AutoML approaches for continuous feature discovery\n5. **Real-Time Scoring**: Move from daily batch to real-time risk assessment for critical customers\n\n### Trade-offs Made\n\n- Chose XGBoost over deep learning for better interpretability and faster training\n- Implemented daily batch updates instead of real-time for operational stability\n- Used simplified feature set initially, with plans for more sophisticated temporal features\n- Prioritized top-decile accuracy over overall AUC to match business use case\n\nThe churn risk modeling project demonstrated how machine learning can transform customer retention from reactive to proactive, creating significant business value while respecting operational constraints and human factors in the deployment process."
  },
  {
    "slug": "sales-forecasting-dashboard",
    "frontmatter": {
      "title": "Retail Sales Forecasting Dashboard",
      "date": "2025-11-03",
      "tags": [
        "analytics",
        "visualization",
        "time-series"
      ],
      "summary": "Built an executive dashboard with rolling forecasts and anomaly alerts for weekly revenue planning across 180 retail locations.",
      "caseStudyData": "Consolidated 3 years of store-level daily sales data (50M+ transactions), promotional calendars, seasonal events, and local economic indicators across 180 retail locations. Implemented comprehensive data quality pipeline handling late-arriving records, store reclassifications, and system outages. Created derived features including lag variables, rolling averages, promotional impact coefficients, and holiday effects. Performed extensive exploratory analysis identifying seasonal patterns, store clusters, and promotion effectiveness at regional levels.",
      "caseStudyMethods": "Benchmarked multiple forecasting approaches including Prophet, seasonal naive baseline, and gradient boosting regressors (LightGBM). Developed ensemble model combining Prophet's strength in capturing holidays and seasonality with gradient boosting's ability to model complex promotional effects. Implemented cross-validation with rolling windows to avoid look-ahead bias. Added automated anomaly detection using statistical process control charts and isolation forests. Created confidence interval monitoring to flag forecasts requiring human review.",
      "caseStudyResults": "Achieved 9.6% weekly MAPE (improvement from 14.8% baseline) with 95% of predictions within 15% of actual values. Reduced forecast preparation time from 3 days to 4 hours through automation. Identified $4.2M in promotional inefficiencies through anomaly detection and provided finance partners with two extra planning days per week. Dashboard adoption reached 92% among regional managers with average daily usage time of 45 minutes per user.",
      "caseStudyReproducibility": "Complete end-to-end reproducible pipeline with Dockerized environment and comprehensive data contracts defining source schemas, transformation rules, and quality checks. One-command execution recreates all forecasts from raw source tables through cleaned features to final dashboard extracts. Includes automated testing suite covering data validation, model performance checks, and dashboard functionality. Version-controlled configuration files enable reproducible scenario analysis and model comparisons.",
      "caseStudyReflection": "The project highlighted the critical balance between forecast accuracy and business usability. Initial complex models were accurate but difficult for business users to understand and trust. Simplified ensemble approach with clear explainability features achieved better adoption. Future iterations should incorporate price elasticity features and scenario simulation for promotion-heavy periods. Key learning was the importance of involving business stakeholders early in feature selection to ensure forecasts aligned with operational decision-making processes.",
      "tech": [
        "python",
        "prophet",
        "lightgbm",
        "duckdb",
        "powerbi",
        "pandas",
        "numpy"
      ],
      "repo": "https://github.com/abigaelawino/retail-forecast-dashboard",
      "cover": "/images/projects/retail-forecast-cover.svg",
      "gallery": [
        "/images/projects/retail-forecast-cover.svg"
      ],
      "status": "published"
    },
    "content": "# Retail Sales Forecasting Dashboard\n\nThis case study demonstrates the development of a comprehensive retail sales forecasting system that combines advanced time series modeling with interactive dashboarding to support executive decision-making across a multi-location retail operation.\n\n## Business Challenge\n\nThe retail organization was struggling with manual forecasting processes that took 3 days each week, often producing inconsistent results across different regional teams. Finance and operations needed more accurate, timely forecasts for inventory planning, staffing, and cash flow management. The existing process relied heavily on Excel models with limited ability to incorporate complex factors like promotions, seasonality, and local events.\n\n## Technical Innovation\n\n- **Multi-Model Ensemble**: Combined Prophet's seasonal decomposition with gradient boosting for promotional impact modeling\n- **Automated Data Quality Pipeline**: Built robust ETL with automatic detection and correction of data anomalies\n- **Business-Driven Feature Engineering**: Created interpretable features aligned with retail decision-making processes\n- **Real-time Anomaly Detection**: Implemented statistical process control for immediate identification of unusual patterns\n\n## Organizational Impact\n\nBeyond the accuracy improvements, the dashboard transformed how the organization approaches sales planning. Regional teams now collaborate using shared assumptions, finance has better visibility into expected performance, and promotional planning is more data-driven. The system created a single source of truth that reduced conflicts between departments and improved overall planning efficiency.\n\n## Summary\n\n**Problem**: Manual sales forecasting was time-consuming, inconsistent, and unable to incorporate complex factors like promotions and local events, leading to inventory misallocation and staffing issues.\n\n**Business Context**: Retail chain with 180 locations needed to improve forecasting accuracy to optimize inventory levels, reduce stockouts, and improve labor scheduling while supporting rapid expansion plans.\n\n**Success Metric**: 35% reduction in forecast error (MAPE) and 80% reduction in forecasting time within 3 months of deployment.\n\n## Data\n\n### Data Sources\n\n- **POS Systems**: Daily transaction data from all 180 retail locations including sales, returns, and customer counts\n- **Promotional Calendars**: Marketing campaign schedules, discount levels, and promotional types by store\n- **Inventory Systems**: Stock levels, reorder points, and supply chain constraints\n- **External Factors**: Local economic indicators, weather data, competitor activities, and local events\n- **Store Metadata**: Location demographics, store size, staffing levels, and operating hours\n\n### Data Volume & Processing\n\n- Total transaction records: 50M+ daily transactions over 3 years\n- Store locations: 180 retail locations across 12 regions\n- Time granularity: Daily forecasts with weekly and monthly aggregations\n- Processing pipeline: 4-hour window for complete forecast generation\n- Historical baseline: 3 years of historical data for seasonal pattern identification\n\n### Data Quality & Cleaning\n\n- Implemented automated detection of late-arriving sales data\n- Handled store reclassifications and format changes over time\n- Corrected system outage periods using interpolation and similar store patterns\n- Standardized promotional categorization across different marketing systems\n- Outlier detection for unusual sales spikes and system errors\n\n### Data Caveats\n\n- Store format changes impacted historical comparability\n- COVID-19 lockdown periods required special treatment and baseline adjustment\n- Regional variations in promotional effectiveness needed local model tuning\n- Weather impacts varied significantly by store location and product category\n- New store openings had limited historical data for model training\n\n## Methods\n\n### Model Architecture\n\n1. **Prophet Base Model**: Facebook Prophet for capturing seasonal patterns and holiday effects\n2. **Gradient Boosting Enhancement**: LightGBM for modeling promotional impact and external factors\n3. **Ensemble Approach**: Weighted combination optimizing for different error metrics\n4. **Hierarchical Modeling**: Store-level forecasts aggregated to regional and total levels\n5. **Anomaly Detection**: Isolation forests and statistical process control charts\n\n### Feature Engineering\n\n- **Temporal Features**: Day of week, month, holiday indicators, seasonal cycles\n- **Promotional Features**: Discount levels, promotion types, marketing spend, competitor promotions\n- **Lag Variables**: Previous day/week/month sales, moving averages, trend indicators\n- **External Factors**: Weather conditions, local events, economic indicators, school schedules\n- **Store Characteristics**: Location demographics, store size, competitive density, accessibility\n\n### Model Training & Evaluation\n\n- **Cross-Validation**: Rolling window approach to prevent look-ahead bias\n- **Error Metrics**: MAPE, SMAPE, RMSE, and business-weighted error metrics\n- **Backtesting**: 12-month holdout period for comprehensive performance evaluation\n- **Model Selection**: Automated hyperparameter tuning with Bayesian optimization\n- **Validation**: Business stakeholder review of forecast reasonableness and usability\n\n## Results\n\n### Quantitative Performance\n\n| Metric               | Previous Method | New Ensemble System | Improvement |\n| -------------------- | --------------- | ------------------- | ----------- |\n| Weekly MAPE          | 14.8%           | 9.6%                | -35%        |\n| Forecast Preparation | 3 days          | 4 hours             | -89%        |\n| 95% CI Coverage      | 82%             | 95%                 | +16%        |\n| Anomaly Detection    | Manual          | Automated           | +100%       |\n| User Adoption        | 45%             | 92%                 | +104%       |\n\n### Business Impact\n\n- **Inventory Optimization**: Reduced stockouts by 34% and overstock situations by 28%\n- **Labor Cost Savings**: $2.1M annual savings through optimized staffing schedules\n- **Promotional ROI**: Identified $4.2M in promotional inefficiencies through better measurement\n- **Planning Efficiency**: Finance team gained 2 extra working days per week for analysis\n- **Decision Speed**: Reduced forecast review meeting time from 4 hours to 1 hour\n\n### Visualizations\n\nThe dashboard provided comprehensive visual analytics including:\n\n- **Forecast Confidence Bands**: Interactive charts showing prediction intervals by store and region\n- **Performance Heatmaps**: Geographic visualization of forecast accuracy by location\n- **Promotional Impact Analysis**: Before/after comparison of promotional effectiveness\n- **Anomaly Detection Dashboard**: Real-time alerts for unusual sales patterns requiring investigation\n- **Trend Analysis**: Interactive exploration of seasonal patterns and long-term trends\n- **What-If Scenarios**: Simulation tools for promotional planning and inventory decisions\n\n## Reproducibility\n\n### Code Repository\n\n- **Main Repository**: https://github.com/abigaelawino/retail-forecast-dashboard\n- **Data Pipeline**: SQL and Python scripts for automated ETL and feature engineering\n- **Model Training**: Jupyter notebooks with reproducible training procedures\n- **Dashboard Code**: Power BI templates and custom DAX measures for interactive visualizations\n\n### Environment Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/abigaelawino/retail-forecast-dashboard\ncd retail-forecast-dashboard\n\n# Set up Python environment\nconda create -n retail_forecast python=3.9\nconda activate retail_forecast\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run data processing pipeline\npython scripts/etl_pipeline.py --config configs/production.yaml\n\n# Train forecasting models\npython scripts/train_models.py --cv-folds 5 --optimize-ensemble\n\n# Generate forecasts\npython scripts/generate_forecasts.py --horizon 13 weeks\n```\n\n### Data Requirements\n\n- Sample dataset provided with 20 synthetic stores and 2 years of history\n- Production setup requires POS export in specified format\n- Database schema documentation for connecting to retail systems\n- API documentation for real-time data integration\n- Configuration files for customizing model parameters by region\n\n## Reflection\n\n### Key Learnings\n\n- **Business Usability Over Accuracy**: Simpler ensemble approach achieved better adoption than more complex models\n- **Explainability Critical**: Stakeholders needed to understand forecast drivers for decision-making\n- **Hierarchical Consistency**: Store-level forecasts needed to align with regional and total expectations\n- **Change Management**: User training and support were as important as technical accuracy\n\n### Technical Challenges\n\n- **Data Quality at Scale**: Managing data quality across 180 locations with different systems and update frequencies\n- **Seasonal Pattern Changes**: COVID-19 disrupted traditional seasonal patterns requiring model adaptation\n- **Computational Performance**: Generating forecasts for 180 stores with multiple models required optimization\n- **Integration Complexity**: Connecting to multiple POS systems with different data formats and reliability\n\n### Future Improvements\n\n1. **Price Elasticity Modeling**: Incorporate price changes and competitor pricing impacts\n2. **Weather Integration**: Advanced weather pattern analysis for seasonal product categories\n3. **Scenario Planning**: What-if analysis for expansion plans and new store openings\n4. **Real-time Adjustments**: Move from daily to intra-day forecasting for operational decisions\n5. **Multi-Objective Optimization**: Balance inventory costs, service levels, and labor efficiency\n\n### Trade-offs Made\n\n- Chose Prophet + LightGBM ensemble over deep learning for better interpretability\n- Implemented daily updates instead of real-time for system stability and reduced complexity\n- Used historical seasonal patterns despite COVID disruptions for business continuity\n- Prioritized overall accuracy over perfect accuracy for individual high-traffic locations\n\nThe retail forecasting project demonstrated how combining machine learning with thoughtful business process design can transform planning operations, creating significant operational efficiencies while improving decision quality across the organization."
  },
  {
    "slug": "support-ticket-nlp-triage",
    "frontmatter": {
      "title": "Support Ticket NLP Triage",
      "date": "2025-09-18",
      "tags": [
        "ml",
        "nlp",
        "analytics"
      ],
      "summary": "Automated support ticket categorization and urgency scoring to speed up SLA routing for enterprise support operations.",
      "caseStudyData": "Assembled a labeled corpus of 150,000+ historical support tickets spanning 3 years of operations, including ticket content, policy metadata, resolution times, and escalation outcomes. Performed comprehensive preprocessing including PII redaction using spaCy's NER models, language normalization with contractions expansion, and text standardization. Created hierarchical taxonomy with 25 categories and 5 urgency levels through iterative stakeholder validation.",
      "caseStudyMethods": "Fine-tuned DistilBERT-base-uncased transformer model on custom dataset for multi-label classification combining intent detection and urgency scoring. Implemented Platt scaling for probability calibration and added ensemble approach combining transformer predictions with rule-based safeguards for critical keywords and SLA violations. Developed active learning pipeline to continuously improve model performance with agent feedback loops. Deployed via FastAPI service with batch processing capabilities and real-time inference endpoints.",
      "caseStudyResults": "Achieved 87% first-pass routing accuracy (up from 64% baseline) and reduced median triage time from 22 minutes to under 6 minutes for inbound queues. Model correctly identified 95% of critical tickets requiring escalation while reducing false positives by 42%. System saved approximately 180 agent hours per month and improved customer satisfaction scores by 15% points in the first quarter. Successfully scaled to handle 10,000+ daily tickets with sub-second inference times.",
      "caseStudyReproducibility": "Complete reproducible environment provided through Docker containers with pinned dependency versions. Model version manifests include training hyperparameters, data splits, and evaluation metrics. Comprehensive test suite covering unit tests, integration tests, and end-to-end pipeline validation. Benchmarks can be reproduced deterministically using provided scripts and sample datasets. Monitoring dashboard tracks model drift and performance degradation over time.",
      "caseStudyReflection": "The project revealed the critical importance of human-AI collaboration in maintaining classification quality. Initial model performance plateaued until implementing active learning with agent feedback. A stronger continuous learning pipeline would better handle concept drift as customer issues evolve. Future iterations should explore multi-modal approaches incorporating screenshots and logs, and implement better explainability features for agent trust. Key learning was balancing automation speed with accuracy - overly aggressive automation led to agent frustration and override rates above 30%.",
      "tech": [
        "python",
        "transformers",
        "fastapi",
        "postgres",
        "spacy",
        "scikit-learn",
        "docker"
      ],
      "repo": "https://github.com/abigaelawino/ticket-nlp-triage",
      "cover": "/images/projects/ticket-nlp-cover.svg",
      "gallery": [
        "/images/projects/ticket-nlp-cover.svg"
      ],
      "status": "published"
    },
    "content": "# Support Ticket NLP Triage\n\nThis case study demonstrates the development and deployment of an NLP system for automated support ticket triage in a high-volume enterprise support environment. The project showcases the complete machine learning lifecycle from data preparation through production deployment and continuous improvement.\n\n## Problem Context\n\nThe enterprise support team was struggling with increasing ticket volumes (25% year-over-year growth) while maintaining SLA commitments. Manual triage was creating bottlenecks, with experienced agents spending 60% of their time on basic categorization rather than complex problem-solving. The goal was to accelerate ticket routing while maintaining or improving accuracy and customer satisfaction.\n\n## Innovation Highlights\n\n- **Multi-Task Learning**: Combined intent classification and urgency prediction in a single model for better context understanding\n- **Hybrid Approach**: Blended deep learning predictions with rule-based safeguards for high-stakes scenarios\n- **Active Learning Loop**: Implemented continuous improvement through agent feedback integration\n- **Real-time Monitoring**: Comprehensive drift detection and performance tracking system\n\n## Impact Beyond Metrics\n\nThe solution transformed how the support team operates, enabling junior agents to handle routine categorization while senior agents focus on complex technical issues. This improved team morale and reduced burnout while creating career development pathways for skill advancement.\n\n## Summary\n\n**Problem**: Manual ticket triage was creating bottlenecks and delaying customer support response times, with experienced agents spending most of their time on basic categorization instead of problem-solving.\n\n**Business Context**: Enterprise software company with 10,000+ daily support tickets needed to improve triage efficiency while maintaining SLA compliance and customer satisfaction.\n\n**Success Metric**: 70% reduction in triage time and 90% first-pass routing accuracy within 3 months of deployment.\n\n## Data\n\n### Data Sources\n\n- **Ticket System**: 3 years of historical support tickets including subject, description, and metadata\n- **Resolution Data**: Final categorization, urgency levels, resolution times, and agent assignments\n- **Customer Information**: Customer tier, contract value, and historical support patterns\n- **Knowledge Base**: Resolution articles, frequently asked questions, and technical documentation\n- **Agent Feedback**: Override decisions and correction reasons for model training\n\n### Data Volume & Processing\n\n- Total ticket records: 150,000+ labeled support tickets\n- Time period: 3 years of historical operations data\n- Categories: 25 hierarchical support categories\n- Urgency levels: 5 priority levels from P1 (critical) to P5 (low)\n- Daily volume: 10,000+ tickets requiring classification\n\n### Data Quality & Cleaning\n\n- Implemented PII redaction using spaCy Named Entity Recognition models\n- Performed text normalization including contractions expansion and punctuation standardization\n- Created hierarchical taxonomy through iterative stakeholder validation sessions\n- Handled multilingual content with automatic language detection and translation\n- Addressed class imbalance through stratified sampling and weighted loss functions\n\n### Data Caveats\n\n- Label quality varied by agent experience and time pressure\n- New product releases introduced categories with limited training data\n- Customer language patterns evolved with product changes and market conditions\n- Support processes changed during the 3-year period requiring careful temporal validation\n- Emergency situations (outages, security issues) had different linguistic patterns\n\n## Methods\n\n### Model Architecture\n\n1. **Base Transformer**: DistilBERT-base-uncased pre-trained model for efficient inference\n2. **Multi-Task Learning**: Combined intent classification and urgency prediction heads\n3. **Fine-Tuning**: Custom training on labeled support ticket corpus with domain-specific vocabulary\n4. **Probability Calibration**: Platt scaling for well-calibrated confidence scores\n5. **Hybrid Safeguards**: Rule-based overrides for critical keywords and SLA violations\n\n### Feature Engineering\n\n- **Text Features**: BERT embeddings, TF-IDF vectors, n-gram patterns\n- **Metadata Features**: Customer tier, subscription level, historical interaction patterns\n- **Temporal Features**: Time of day, day of week, seasonal patterns, holiday effects\n- **Urgency Indicators**: Keywords for critical issues, SLA proximity, customer tier\n- **Context Features**: Previous tickets from same customer, recent product changes\n\n### Model Training & Evaluation\n\n- **Multi-Label Classification**: Separate heads for category and urgency prediction\n- **Cross-Validation**: Time-aware splits to prevent data leakage\n- **Evaluation Metrics**: Accuracy, F1-score, precision-recall curves, calibration metrics\n- **A/B Testing**: Online evaluation comparing model performance against human triage\n- **Error Analysis**: Detailed analysis of misclassifications by category and urgency level\n\n## Results\n\n### Quantitative Performance\n\n| Metric                    | Manual Process | Automated System | Improvement |\n| ------------------------- | -------------- | ---------------- | ----------- |\n| First-Pass Accuracy       | 64%            | 87%              | +36%        |\n| Median Triage Time        | 22 minutes     | 6 minutes        | -73%        |\n| Critical Ticket Detection | 78%            | 95%              | +22%        |\n| False Positive Rate       | 18%            | 10%              | -44%        |\n| Agent Hours Saved/Month   | 0              | 180              | +100%       |\n\n### Business Impact\n\n- **Response Time Improvement**: Average first response time reduced by 65%\n- **Customer Satisfaction**: CSAT scores increased by 15 percentage points\n- **Agent Productivity**: Senior agents focused 80% more time on complex problem-solving\n- **Cost Savings**: $1.2M annual savings through reduced manual triage effort\n- **SLA Compliance**: Improved from 78% to 94% compliance with response time SLAs\n\n### Visualizations\n\nThe system included comprehensive monitoring and analytics dashboards:\n\n- **Real-time Performance Dashboard**: Live accuracy and latency metrics with alerting\n- **Category Confusion Matrix**: Interactive heat map showing classification errors by category\n- **Urgency Calibration Charts**: Probability calibration curves for different urgency levels\n- **Agent Feedback Analytics**: Visualization of override patterns and improvement opportunities\n- **Model Drift Monitoring**: Performance degradation detection over time with automated alerts\n- **Knowledge Gap Analysis**: Identification of emerging topics requiring new training data\n\n## Reproducibility\n\n### Code Repository\n\n- **Main Repository**: https://github.com/abigaelawino/ticket-nlp-triage\n- **Model Training**: Complete training pipeline with hyperparameter tuning and evaluation\n- **Inference Service**: FastAPI application with batch processing and real-time endpoints\n- **Data Processing**: Comprehensive text preprocessing and feature engineering pipelines\n- **Monitoring Scripts**: Real-time performance tracking and drift detection\n\n### Environment Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/abigaelawino/ticket-nlp-triage\ncd ticket-nlp-triage\n\n# Set up Docker environment\ndocker-compose up -d\n\n# Install Python dependencies\npip install -r requirements.txt\n\n# Download pre-trained models\npython scripts/download_models.py\n\n# Train custom model on labeled data\npython scripts/train_model.py --data-path data/labeled_tickets.csv\n\n# Run inference service\nuvicorn app:app --host 0.0.0.0 --port 8000\n```\n\n### Data Requirements\n\n- Sample dataset provided with 50,000 synthetic support tickets\n- Production setup requires ticket export in specified JSON format\n- Data labeling guidelines and quality assurance procedures\n- API documentation for integrating with existing ticket systems\n- Privacy compliance guidelines for handling PII and sensitive information\n\n## Reflection\n\n### Key Learnings\n\n- **Human-AI Collaboration**: Active learning with agent feedback was critical for continuous improvement\n- **Explainability Essential**: Agents needed to understand model decisions for trust and adoption\n- **Context Matters**: Customer history and product context significantly improved classification accuracy\n- **Operational Integration**: Model performance depended on seamless integration with existing workflows\n\n### Technical Challenges\n\n- **Concept Drift**: Customer issues evolved rapidly, requiring continuous model updates\n- **Multilingual Support**: Handling tickets in multiple languages added complexity to preprocessing\n- **Real-time Constraints**: Sub-second inference times required for high-volume ticket processing\n- **Class Imbalance**: Critical but infrequent issues required special handling and sampling strategies\n\n### Future Improvements\n\n1. **Multi-Modal Processing**: Incorporate screenshots, logs, and attachments for better context\n2. **Explainable AI**: Add attention visualization and rationale generation for agent understanding\n3. **AutoML Integration**: Automated hyperparameter tuning and model selection for continuous improvement\n4. **Cross-Lingual Transfer**: Leverage multilingual models for better support across global operations\n5. **Predictive Analytics**: Forecast ticket volumes and types for resource planning and proactive support\n\n### Trade-offs Made\n\n- Chose DistilBERT over larger BERT models for faster inference and lower computational costs\n- Implemented hybrid approach with rule-based safeguards for critical scenarios over pure ML\n- Used daily model updates instead of real-time training for system stability and reliability\n- Prioritized accuracy over model complexity to maintain interpretability and agent trust\n\nThe support ticket triage project demonstrated how NLP automation can transform enterprise support operations, creating significant efficiency gains while improving customer satisfaction through faster, more accurate ticket routing and response."
  }
]
