[
  {
    "slug": "customer-segmentation-dashboard",
    "title": "Customer Segmentation Analytics Dashboard",
    "date": "2026-02-14",
    "tags": [
      "analytics",
      "visualization",
      "dashboard"
    ],
    "summary": "Built an interactive analytics dashboard for customer segmentation using clustering algorithms and real-time data visualization.",
    "caseStudyData": "Analyzed 2.5M customer records with 50+ behavioral and demographic features including purchase history, website engagement patterns, demographic information, and customer support interactions. Integrated data from multiple sources (CRM, web analytics, email platform, POS systems) and performed extensive feature engineering to create meaningful segmentation variables.",
    "caseStudyMethods": "Applied unsupervised machine learning using K-means and hierarchical clustering combined with PCA for dimensionality reduction. Used elbow method, silhouette analysis, and domain expertise to determine optimal cluster count. Implemented real-time segmentation updates using incremental clustering algorithms. Built interactive dashboards with drill-down capabilities and automated insights generation.",
    "caseStudyResults": "Identified 6 distinct customer segments with clear business characteristics, leading to 42% improvement in targeted marketing ROI. Reduced customer acquisition cost by 28% through optimized channel allocation. Dashboard adoption by 95% of marketing team with 4.7/5 user satisfaction score. Real-time alerts identified 15 high-value customer churn risks weekly.",
    "caseStudyReproducibility": "Complete Jupyter notebooks with data preprocessing, model training, and evaluation. Docker environment with all dependencies including scikit-learn, plotly, and dashboard framework. SQL scripts for data extraction and transformation. Documentation for deploying dashboard using Docker Compose with automated data pipeline updates.",
    "caseStudyReflection": "Key insight was that behavioral features outperformed demographic data for meaningful segmentation. Challenge was balancing statistical cluster validity with business interpretability - some mathematically optimal clusters were too complex for marketing campaigns. Next iteration should incorporate temporal segmentation to capture customer lifecycle changes and use deep learning for automatic feature extraction.",
    "tech": [
      "python",
      "plotly",
      "dash",
      "postgres",
      "redis",
      "scikit-learn",
      "pandas",
      "numpy",
      "sqlalchemy",
      "docker"
    ],
    "repo": "https://github.com/abigaelawino/customer-segmentation-dashboard",
    "cover": "/images/projects/segmentation-dashboard-cover.svg",
    "status": "published"
  },
  {
    "slug": "ecommerce-recommendation-engine",
    "title": "E-Commerce Product Recommendation Engine",
    "date": "2026-02-14",
    "tags": [
      "ml",
      "recommendation",
      "analytics"
    ],
    "summary": "Developed a hybrid recommendation system combining collaborative filtering and content-based approaches to increase cross-selling and customer engagement.",
    "caseStudyData": "Processed 3.2M user interactions including clicks, purchases, wishlists, and cart events over 18 months. Integrated product catalog with 50K+ items including categorical features, text descriptions, and image embeddings. Performed extensive data cleaning to remove bots, handle cold-start users, and normalize implicit feedback signals.",
    "caseStudyMethods": "Implemented hybrid recommendation approach combining matrix factorization (ALS) for collaborative filtering with TF-IDF and neural embeddings for content-based filtering. Used multi-armed bandit exploration for cold-start problems and incorporated temporal dynamics to capture changing user preferences. Evaluated using offline metrics (precision@K, recall@K, MAP) and online A/B testing with business metrics.",
    "caseStudyResults": "Achieved 38% improvement in click-through rate and 27% increase in conversion rate for recommended products. Reduced cold-start problem impact by 62% through hybrid approach. System generated $4.2M additional revenue in first 6 months through improved product discovery. User session duration increased by 45% when recommendations were prominently displayed.",
    "caseStudyReproducibility": "Complete pipeline available with Apache Spark for data processing, TensorFlow for neural embeddings, and Flask API for serving recommendations. Includes Docker compose setup for local development, comprehensive unit tests, and monitoring dashboards for model performance tracking. All hyperparameters and experiment logs stored in MLflow for reproducibility.",
    "caseStudyReflection": "Key challenge was balancing exploration vs exploitation in recommendations while maintaining diversity. Next iteration should incorporate real-time contextual signals and implement graph neural networks for better item relationships. Learned importance of business metrics over pure accuracy - focusing on revenue impact rather than just offline metrics drove better adoption.",
    "tech": [
      "python",
      "spark",
      "tensorflow",
      "flask",
      "kafka",
      "mysql",
      "redis",
      "pandas",
      "numpy",
      "scikit-learn"
    ],
    "repo": "https://github.com/abigaelawino/recommendation-engine",
    "cover": "/images/projects/recommendation-engine-cover.svg",
    "status": "published"
  },
  {
    "slug": "customer-churn-case-study",
    "title": "Customer Churn Risk Modeling",
    "date": "2026-01-12",
    "tags": [
      "ml",
      "analytics",
      "time-series"
    ],
    "summary": "Built a churn risk model and dashboard to prioritize retention outreach for B2B SaaS customers.",
    "caseStudyData": "Combined 24 months of CRM activity, subscription transactions, and support logs for 10,000+ customers. Performed extensive data cleaning including standardizing customer IDs across systems, handling missing engagement metrics through multiple imputation, and creating temporal features to capture usage trends over time.",
    "caseStudyMethods": "Trained gradient-boosted trees (XGBoost) with recency-frequency-monetary (RFM) features, including login frequency, feature usage patterns, support ticket history, and payment behavior. Compared against logistic regression baseline using AUC-ROC and precision-recall metrics. Implemented cost-sensitive learning to account for different retention costs across customer segments and tuned classification thresholds based on retention team capacity constraints.",
    "caseStudyResults": "Achieved 0.84 AUC-ROC (compared to 0.71 baseline) and improved top-decile churn capture by 31% over existing heuristic approach. The model reduced wasted outreach by 47% by focusing account manager effort on high-risk cohorts. Dashboard enabled proactive retention campaigns that saved an estimated $2.3M in ARR over 6 months through targeted interventions.",
    "caseStudyReproducibility": "Repository includes complete reproducible pipeline with pinned dependencies via requirements.txt, SQL feature extraction scripts with proper data lineage tracking, and Jupyter notebooks with parity checks to ensure production model matches training results. Docker environment provided for consistent reproducibility across development and production environments.",
    "caseStudyReflection": "The biggest challenge was balancing model accuracy with operational constraints of the retention team. Next iteration should incorporate causal uplift testing to separate true intervention impact from naturally reactivating customers, and explore multi-armed bandit approaches for retention offer optimization. Also learned the importance of early collaboration with business stakeholders to define success metrics aligned with operational capabilities.",
    "tech": [
      "python",
      "xgboost",
      "postgres",
      "tableau",
      "scikit-learn",
      "pandas"
    ],
    "repo": "https://github.com/abigaelawino/churn-risk-model",
    "cover": "/images/projects/churn-risk-cover.svg",
    "status": "published"
  },
  {
    "slug": "sales-forecasting-dashboard",
    "title": "Retail Sales Forecasting Dashboard",
    "date": "2025-11-03",
    "tags": [
      "analytics",
      "visualization",
      "time-series"
    ],
    "summary": "Built an executive dashboard with rolling forecasts and anomaly alerts for weekly revenue planning across 180 retail locations.",
    "caseStudyData": "Consolidated 3 years of store-level daily sales data (50M+ transactions), promotional calendars, seasonal events, and local economic indicators across 180 retail locations. Implemented comprehensive data quality pipeline handling late-arriving records, store reclassifications, and system outages. Created derived features including lag variables, rolling averages, promotional impact coefficients, and holiday effects. Performed extensive exploratory analysis identifying seasonal patterns, store clusters, and promotion effectiveness at regional levels.",
    "caseStudyMethods": "Benchmarked multiple forecasting approaches including Prophet, seasonal naive baseline, and gradient boosting regressors (LightGBM). Developed ensemble model combining Prophet's strength in capturing holidays and seasonality with gradient boosting's ability to model complex promotional effects. Implemented cross-validation with rolling windows to avoid look-ahead bias. Added automated anomaly detection using statistical process control charts and isolation forests. Created confidence interval monitoring to flag forecasts requiring human review.",
    "caseStudyResults": "Achieved 9.6% weekly MAPE (improvement from 14.8% baseline) with 95% of predictions within 15% of actual values. Reduced forecast preparation time from 3 days to 4 hours through automation. Identified $4.2M in promotional inefficiencies through anomaly detection and provided finance partners with two extra planning days per week. Dashboard adoption reached 92% among regional managers with average daily usage time of 45 minutes per user.",
    "caseStudyReproducibility": "Complete end-to-end reproducible pipeline with Dockerized environment and comprehensive data contracts defining source schemas, transformation rules, and quality checks. One-command execution recreates all forecasts from raw source tables through cleaned features to final dashboard extracts. Includes automated testing suite covering data validation, model performance checks, and dashboard functionality. Version-controlled configuration files enable reproducible scenario analysis and model comparisons.",
    "caseStudyReflection": "The project highlighted the critical balance between forecast accuracy and business usability. Initial complex models were accurate but difficult for business users to understand and trust. Simplified ensemble approach with clear explainability features achieved better adoption. Future iterations should incorporate price elasticity features and scenario simulation for promotion-heavy periods. Key learning was the importance of involving business stakeholders early in feature selection to ensure forecasts aligned with operational decision-making processes.",
    "tech": [
      "python",
      "prophet",
      "lightgbm",
      "duckdb",
      "powerbi",
      "pandas",
      "numpy"
    ],
    "repo": "https://github.com/abigaelawino/retail-forecast-dashboard",
    "cover": "/images/projects/retail-forecast-cover.svg",
    "status": "published"
  },
  {
    "slug": "support-ticket-nlp-triage",
    "title": "Support Ticket NLP Triage",
    "date": "2025-09-18",
    "tags": [
      "ml",
      "nlp",
      "analytics"
    ],
    "summary": "Automated support ticket categorization and urgency scoring to speed up SLA routing for enterprise support operations.",
    "caseStudyData": "Assembled a labeled corpus of 150,000+ historical support tickets spanning 3 years of operations, including ticket content, policy metadata, resolution times, and escalation outcomes. Performed comprehensive preprocessing including PII redaction using spaCy's NER models, language normalization with contractions expansion, and text standardization. Created hierarchical taxonomy with 25 categories and 5 urgency levels through iterative stakeholder validation.",
    "caseStudyMethods": "Fine-tuned DistilBERT-base-uncased transformer model on custom dataset for multi-label classification combining intent detection and urgency scoring. Implemented Platt scaling for probability calibration and added ensemble approach combining transformer predictions with rule-based safeguards for critical keywords and SLA violations. Developed active learning pipeline to continuously improve model performance with agent feedback loops. Deployed via FastAPI service with batch processing capabilities and real-time inference endpoints.",
    "caseStudyResults": "Achieved 87% first-pass routing accuracy (up from 64% baseline) and reduced median triage time from 22 minutes to under 6 minutes for inbound queues. Model correctly identified 95% of critical tickets requiring escalation while reducing false positives by 42%. System saved approximately 180 agent hours per month and improved customer satisfaction scores by 15% points in the first quarter. Successfully scaled to handle 10,000+ daily tickets with sub-second inference times.",
    "caseStudyReproducibility": "Complete reproducible environment provided through Docker containers with pinned dependency versions. Model version manifests include training hyperparameters, data splits, and evaluation metrics. Comprehensive test suite covering unit tests, integration tests, and end-to-end pipeline validation. Benchmarks can be reproduced deterministically using provided scripts and sample datasets. Monitoring dashboard tracks model drift and performance degradation over time.",
    "caseStudyReflection": "The project revealed the critical importance of human-AI collaboration in maintaining classification quality. Initial model performance plateaued until implementing active learning with agent feedback. A stronger continuous learning pipeline would better handle concept drift as customer issues evolve. Future iterations should explore multi-modal approaches incorporating screenshots and logs, and implement better explainability features for agent trust. Key learning was balancing automation speed with accuracy - overly aggressive automation led to agent frustration and override rates above 30%.",
    "tech": [
      "python",
      "transformers",
      "fastapi",
      "postgres",
      "spacy",
      "scikit-learn",
      "docker"
    ],
    "repo": "https://github.com/abigaelawino/ticket-nlp-triage",
    "cover": "/images/projects/ticket-nlp-cover.svg",
    "status": "published"
  }
]
